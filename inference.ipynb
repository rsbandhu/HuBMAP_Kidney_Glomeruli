{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler #don't need\n",
    "#add other imports only as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nput the helper function rle_to_2d and 2d_to_rle over here\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "put the helper function rle_to_2d and 2d_to_rle over here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padsize(img, reduce, sz):\n",
    "\n",
    "    shape = img.shape\n",
    "    print(shape)\n",
    "\n",
    "    pad0 = (reduce*sz - shape[0] % (reduce*sz)) % (reduce*sz)\n",
    "    pad1 = (reduce*sz - shape[1] % (reduce*sz)) % (reduce*sz)\n",
    "    pad_x = (pad0//2, pad0-pad0//2)\n",
    "    pad_y = (pad1//2, pad1-pad1//2)\n",
    "\n",
    "    return pad_x, pad_y\n",
    "\n",
    "def check_threshold(img_BGR, sat_threshold, pixcount_th):\n",
    "\n",
    "    \"\"\"\n",
    "    checks if an input image passes the threshold conditions:\n",
    "    conditions:\n",
    "    not black--> sum of pixels exceeed a threshold = pixcount_th\n",
    "    saturation --> number of pixels with saturation > sat_threshold exceeds pixcount_th\n",
    "    Returns:\n",
    "    True if both conditions are met else False\n",
    "    \"\"\"\n",
    "    #if most of the pixels are black, return False\n",
    "    #edge of each image is typically black\n",
    "    if img_BGR.sum() < pixcount_th:\n",
    "        return False\n",
    "\n",
    "    #convert to hue, saturation, Value in openCV\n",
    "    hsv = cv2.cvtColor(img_BGR, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    # if less than prefined number of values are above a saturation threshold, return False\n",
    "    #this is typically the gray background around the biological object\n",
    "    if (s > sat_threshold).sum() < pixcount_th:\n",
    "        return False\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image():\n",
    "    def __init__(self, img, img_name =None):\n",
    "        self.img = img\n",
    "        self.shape = img.shape\n",
    "        self.name = img_name\n",
    "\n",
    "        self.image_reshape()\n",
    "        self.dx = self.shape[0]\n",
    "        self.dy = self.shape[1]\n",
    "\n",
    "        self.tile_size = None\n",
    "\n",
    "        self.pad_x = None\n",
    "        self.pad_y = None\n",
    "        self.tiled_img = None\n",
    "\n",
    "        self.mask_rle = None\n",
    "        self.mask_2d = None\n",
    "        self.tiled_mask = None\n",
    "        \n",
    "    def image_reshape(self):\n",
    "    \n",
    "        if len(self.shape) == 5:\n",
    "            self.img = np.transpose(self.img.squeeze(), (1, 2, 0))\n",
    "            self.shape = self.img.shape\n",
    "            \n",
    "    \n",
    "    def split_image_mask_into_tiles(self, reduce=1, sz=512):\n",
    "     \n",
    "        self.tile_size = sz\n",
    "\n",
    "        self.pad_x, self.pad_y = utils.get_padsize(self.img, reduce, sz)\n",
    "        print(self.pad_x, self.pad_y)\n",
    "        #Create padded Image and padded mask2D\n",
    "        img_padded  = np.pad(self.img, [self.pad_x, self.pad_y, (0, 0)], constant_values=0)\n",
    "        mask_padded = np.pad(self.mask_2d, [self.pad_x, self.pad_y], constant_values = 0)\n",
    "\n",
    "        print(\"shape of image after padding:: \", img_padded.shape,\n",
    "            img_padded.shape[0]//sz, img_padded.shape[1]//sz)\n",
    "\n",
    "        print(\"shape of mask after padding:: \", mask_padded.shape,\n",
    "              mask_padded.shape[0]//sz, mask_padded.shape[1]//sz)\n",
    "\n",
    "        #tile the padded image\n",
    "        img_reshaped = img_padded.reshape(\n",
    "            img_padded.shape[0]//sz, sz, img_padded.shape[1]//sz, sz, 3)\n",
    "        img_reshaped = img_reshaped.transpose(0, 2, 1, 3, 4).reshape(-1, sz, sz, 3)\n",
    "\n",
    "        #tile the padded mask2D\n",
    "        mask_reshaped = mask_padded.reshape(\n",
    "            mask_padded.shape[0]//sz, sz, mask_padded.shape[1]//sz, sz)\n",
    "        mask_reshaped = mask_reshaped.transpose(\n",
    "            0, 2, 1, 3).reshape(-1, sz, sz)\n",
    "\n",
    "        self.tiled_img = img_reshaped\n",
    "        self.tiled_mask = mask_reshaped\n",
    "        \n",
    "    \n",
    "    def save_thresholded_image(self, tiled_threshold_img_dir, mask_tile_dict, sat_threshold=40, pixcount_th=200):\n",
    "        \"\"\"\n",
    "        instead of save, check thresholding of image\n",
    "        if it passes threshold then do an inference else predict an mask of all zeros\n",
    "        \"\"\"\n",
    "        \n",
    "        n = self.tiled_img.shape[0]\n",
    "\n",
    "        valid_img_count = 0\n",
    "        valid_idx = []\n",
    "        print(f\"Original tiled image count = {n}\")\n",
    "\n",
    "        for i in range(n):\n",
    "            img_BGR = self.tiled_img[i, :, :, :]\n",
    "            if utils.check_threshold(img_BGR, sat_threshold, pixcount_th):\n",
    "                valid_img_count += 1\n",
    "                valid_idx.append(i)\n",
    "                \n",
    "                #create an id for the image tile\n",
    "                img_tile_id = f\"{self.name}_{str(self.tile_size)}_{str(valid_img_count)}_{str(i)}\"\n",
    "                img_name = img_tile_id+'.png'  # name of the saved image tile\n",
    "\n",
    "                mask_for_tile = self.tiled_mask[i, :, :]  # get the mask for the tile\n",
    "                #convert the mask for the tile to rle\n",
    "                mask_rle = self.mask_2d_to_rle(mask_for_tile)\n",
    "                #save the rle mask to a dict, key = name of the corresponding image tile\n",
    "                mask_tile_dict[img_tile_id] = mask_rle\n",
    "\n",
    "                #if valid_img_count == 1001:\n",
    "                cv2.imwrite(os.path.join(tiled_threshold_img_dir, img_name), img_BGR)\n",
    "\n",
    "        print(f\"Image count after thresholding = {valid_img_count}\")\n",
    "\n",
    "\n",
    "    def mask_rle_to_2d(self):\n",
    "        \"\"\"\n",
    "        converts mask from run length encoding to 2D numpy array\n",
    "        \"\"\"\n",
    "        dx = self.dx\n",
    "        dy = self.dy\n",
    "\n",
    "    \n",
    "        mask = np.zeros(dx*dy, dtype=np.uint8)\n",
    "        s = self.mask_rle.split()  # split the rle encoding\n",
    "        for i in range(len(s)//2):\n",
    "            start = int(s[2*i])-1\n",
    "            length = int(s[2*i+1])\n",
    "            mask[start:start+length] = 1\n",
    "        self.mask_2d = mask.reshape(dy, dx).T\n",
    "        \n",
    "        self.mask_2d = utils.mask_rle_to_2d(self.mask_rle, dx, dy)\n",
    "        \n",
    "    \n",
    "    def mask_2d_to_rle(self, mask_2d):\n",
    "        \"\"\"\n",
    "        Takes a 2D mask of 0/1 and returns the run length encoded form\n",
    "        \"\"\"\n",
    "\n",
    "        mask = mask_2d.T.reshape(-1)  # order by columns and flatten to 1D\n",
    "        mask_padded = np.pad(mask, 1)  # pad zero on both sides\n",
    "        #find the start positions of the 1's\n",
    "        starts = np.where((mask_padded[:-1] == 0) & (mask_padded[1:] == 1))[0]\n",
    "        #find the end positions of 1's for each run\n",
    "        ends = np.where((mask_padded[:-1] == 1) & (mask_padded[1:] == 0))[0]\n",
    "\n",
    "        rle = np.zeros(2*len(starts))\n",
    "        \n",
    "        rle[::2] = starts\n",
    "        #length of each run = end position - start position\n",
    "        rle[1::2] = ends - starts\n",
    "        rle = rle.astype(int)\n",
    "        return rle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## upload model folder - Rudra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __str__(self):\n",
    "        model_params = filter(lambda x: x.requires_grad, self.parameters())\n",
    "\n",
    "        return super(BaseModel, self).__str__()\n",
    "\n",
    "\n",
    "class Conv2x(nn.Module):\n",
    "    '''\n",
    "    preserves the the size of the image\n",
    "    '''\n",
    "    def __init__(self, in_ch, out_ch, inner_ch=None):\n",
    "        super(Conv2x, self).__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.inner_ch = out_ch//2 if inner_ch is None else inner_ch\n",
    "\n",
    "        self.conv2d_1 = nn.Conv2d(self.in_ch, self.inner_ch,\n",
    "                                  kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2d_2 = nn.Conv2d(self.inner_ch, self.out_ch,\n",
    "                                  kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inner_ch)\n",
    "        self.bn2 = nn.BatchNorm2d(self.out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(encoder, self).__init__()\n",
    "        self.conv2x = Conv2x(in_ch, out_ch)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, ceil_mode=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2x(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(decoder, self).__init__()\n",
    "        self.transposeconv = nn.ConvTranspose2d(\n",
    "            in_ch, in_ch//2, kernel_size=2, stride=2)\n",
    "        self.conv2x = Conv2x(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x_down, x_up, interpolate=True):\n",
    "\n",
    "        x_up = self.transposeconv(x_up)\n",
    "\n",
    "        #check for matching dims before concatenating\n",
    "\n",
    "        if (x_up.size(2) != x_up.size(2)) or (x_up.size(3) != x_up.size(3)):\n",
    "            if interpolate:\n",
    "                x_up = F.interpolate(x_up, size=(x_down.size(2), x_down.size(3)),\n",
    "                mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "        #Concat features from down conv channel and current up-conv\n",
    "        #along channel dim =1\n",
    "        x_up = torch.cat([x_up, x_down], dim=1) \n",
    "        x_up = self.conv2x(x_up)\n",
    "\n",
    "        return x_up\n",
    "\n",
    "class UNet(BaseModel):\n",
    "\n",
    "    def __init__(self, in_ch=3, conv_channels=[16, 32, 64, 128, 256]):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.conv_channels = conv_channels\n",
    "        self.conv_start = Conv2x(in_ch, conv_channels[0]) #output_size = input_size\n",
    "        self.down1 = encoder(conv_channels[0], conv_channels[1])   #output_size = input_size/2\n",
    "        self.down2 = encoder(conv_channels[1], conv_channels[2])   #output_size = input_size/2\n",
    "        self.down3 = encoder(conv_channels[2], conv_channels[3])   #output_size = input_size/2\n",
    "        self.down4 = encoder(conv_channels[3], conv_channels[4])   #output_size = input_size/2\n",
    "\n",
    "        self.conv_middle = Conv2x(conv_channels[4], conv_channels[4]) #output_size = input_size\n",
    "\n",
    "        self.up4 = decoder(conv_channels[4], conv_channels[3]) #output_size = input_size * 2\n",
    "        self.up3 = decoder(conv_channels[3], conv_channels[2]) #output_size = input_size * 2\n",
    "        self.up2 = decoder(conv_channels[2], conv_channels[1]) #output_size = input_size * 2\n",
    "        self.up1 = decoder(conv_channels[1], conv_channels[0]) #output_size = input_size * 2\n",
    "\n",
    "        self.final_conv = nn.Conv2d(self.conv_channels[0], 1, kernel_size=1)\n",
    "\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                module.weight.data.fill_(1)\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # size of x = [B, _, nx, ny]\n",
    "        \n",
    "        x1 = self.conv_start(x)  # size of x = [B, self.conv_channels[0], nx, ny]\n",
    "        x2 = self.down1(x1)  # size of x = [B, self.conv_channels[1], nx/2, ny/2]\n",
    "        x3 = self.down2(x2)  # size of x = [B, self.conv_channels[2], nx/4, ny/4]\n",
    "        x4 = self.down3(x3)  # size of x = [B, self.conv_channels[3], nx/8, ny/8]\n",
    "        x5 = self.down4(x4)  # size of x = [B, self.conv_channels[4], nx/16, ny/16]\n",
    "\n",
    "        x = self.conv_middle(x5)  # size of x = [B, self.conv_channels[4], nx/16, ny/16]\n",
    "\n",
    "        x = self.up4(x4, x)       # size of x = [B, self.conv_channels[3], nx/8, ny/8]\n",
    "        x = self.up3(x3, x)       # size of x = [B, self.conv_channels[2], nx/4, ny/4]\n",
    "        x = self.up2(x2, x)       # size of x = [B, self.conv_channels[1], nx/2, ny/2]\n",
    "        x = self.up1(x1, x)       # size of x = [B, self.conv_channels[0], nx, ny]\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the base model class and classes in unet.py . Do not change anything - Ethan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload the checkpoint - Rudra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_dice_iou(output, target, smooth = 0.005):\n",
    "        tp = (output * target).sum(axis=(1,2)) #intersection\n",
    "        fp = (output * (1.0 - target)).sum(axis=(1,2)) #false positives\n",
    "        fn = ((1.0 - output) * target).sum(axis=(1,2)) #false negatives\n",
    "        dice = np.mean((2.0 * tp + smooth) / (2 * tp + fp + fn + smooth))\n",
    "        iou = np.mean((tp + smooth) / (tp + fp + fn + smooth))\n",
    "\n",
    "        return dice, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ineference_single_image(image):\n",
    "        if self.val_loader is None:\n",
    "            print(f\"No val loader exists\")\n",
    "            return {}\n",
    "\n",
    "        model = unet.UNet()\n",
    "        \n",
    "        #resume checkpoint , load checkpoint\n",
    "        #find where to store the model checkpoint\n",
    "        \n",
    "        self.model.eval()\n",
    "        self._reset_metrics()\n",
    "        tbar = tqdm(self.val_loader, ncols=100)\n",
    "        \n",
    "        #read an image\n",
    "        img_raw = tiff.imread(os.path.join(datadir_train, f))\n",
    "        \n",
    "        #instantiate image class\n",
    "        Image(img_raw)\n",
    "        #call Image.split_image_mask_into_tiles(self, reduce=1, sz=512) to create\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #loop over the tiles\n",
    "            for i in range(tile_count)\n",
    "            #for i, sample_batch in enumerate(tbar):\n",
    "                #for each tile\n",
    "                #check thresolhold\n",
    "                #if thresold passes, then do inference\n",
    "                img = sample_batch['image']\n",
    "                #mask = sample_batch['mask'].float()\n",
    "\n",
    "                batch_size = img.shape[0]\n",
    "                img = img.to(self.device)\n",
    "                #mask = mask.to(self.device)\n",
    "\n",
    "                out = torch.squeeze(self.model(img), 1)\n",
    "                \n",
    "        \n",
    "                #accumulate the predictions into a big np array\n",
    "                #prediction size = [total_tiles, sz, sz]\n",
    "                \n",
    "        # call Image.reconstruct_original_from_padded_tiled_image(self, tiled_predicted_mask)\n",
    "        # to get the predicted mask as the same size of the original image / mask\n",
    "        \n",
    "        #calculate metrics\n",
    "        #calculate 2d_to_rle\n",
    "        \n",
    "            \n",
    "\n",
    "        return mask_pred_2d_to_rle\n",
    "\n",
    "#call  this function for all the prediction images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resume_checkpoint(self, checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path) #need this line\n",
    "\n",
    "        last_epoch = checkpoint['epoch']\n",
    "        model_name = checkpoint['config']['name']\n",
    "\n",
    "        if model_name == self.config['name']:\n",
    "            self.model.load_state_dict(checkpoint['state_dict']) #need this line\n",
    "            self.start_epoch = last_epoch + 1\n",
    "            return True\n",
    "        else:\n",
    "            print(\"current model name doesn't match with previously saved model name !!\")\n",
    "            print(\"Currennt model name: {} , previous model name : {}\".format(self.config['name'], model_name))\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on one image we trained\n",
    "#comapre the prediction to mask label\n",
    "#check prediction dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.atlassian.com/git/tutorials\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
