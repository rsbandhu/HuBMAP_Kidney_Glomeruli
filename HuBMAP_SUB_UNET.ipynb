{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import tifffile as tiff\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_rle_to_2d(self):\n",
    "        \"\"\"\n",
    "        converts mask from run length encoding to 2D numpy array\n",
    "        \"\"\"\n",
    "        dx = self.dx\n",
    "        dy = self.dy\n",
    "\n",
    "    \n",
    "        mask = np.zeros(dx*dy, dtype=np.uint8)\n",
    "        s = self.mask_rle.split()  # split the rle encoding\n",
    "        for i in range(len(s)//2):\n",
    "            start = int(s[2*i])-1\n",
    "            length = int(s[2*i+1])\n",
    "            mask[start:start+length] = 1\n",
    "        self.mask_2d = mask.reshape(dy, dx).T\n",
    "        \n",
    "        self.mask_2d = utils.mask_rle_to_2d(self.mask_rle, dx, dy)\n",
    "        \n",
    "    \n",
    "def mask_2d_to_rle(self, mask_2d):\n",
    "        \"\"\"\n",
    "        Takes a 2D mask of 0/1 and returns the run length encoded form\n",
    "        \"\"\"\n",
    "\n",
    "        mask = mask_2d.T.reshape(-1)  # order by columns and flatten to 1D\n",
    "        mask_padded = np.pad(mask, 1)  # pad zero on both sides\n",
    "        #find the start positions of the 1's\n",
    "        starts = np.where((mask_padded[:-1] == 0) & (mask_padded[1:] == 1))[0]\n",
    "        #find the end positions of 1's for each run\n",
    "        ends = np.where((mask_padded[:-1] == 1) & (mask_padded[1:] == 0))[0]\n",
    "\n",
    "        rle = np.zeros(2*len(starts))\n",
    "        \n",
    "        rle[::2] = starts\n",
    "        #length of each run = end position - start position\n",
    "        rle[1::2] = ends - starts\n",
    "        rle = rle.astype(int)\n",
    "        return rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padsize(img, reduce, sz):\n",
    "\n",
    "    shape = img.shape\n",
    "    print(shape)\n",
    "\n",
    "    pad0 = (reduce*sz - shape[0] % (reduce*sz)) % (reduce*sz)\n",
    "    pad1 = (reduce*sz - shape[1] % (reduce*sz)) % (reduce*sz)\n",
    "    pad_x = (pad0//2, pad0-pad0//2)\n",
    "    pad_y = (pad1//2, pad1-pad1//2)\n",
    "\n",
    "    return pad_x, pad_y\n",
    "\n",
    "\n",
    "def check_threshold(img_BGR, sat_threshold, pixcount_th):\n",
    "\n",
    "    \"\"\"\n",
    "    checks if an input image passes the threshold conditions:\n",
    "    conditions:\n",
    "    not black--> sum of pixels exceeed a threshold = pixcount_th\n",
    "    saturation --> number of pixels with saturation > sat_threshold exceeds pixcount_th\n",
    "    Returns:\n",
    "    True if both conditions are met else False\n",
    "    \"\"\"\n",
    "    #if most of the pixels are black, return False\n",
    "    #edge of each image is typically black\n",
    "    if img_BGR.sum() < pixcount_th:\n",
    "        return False\n",
    "\n",
    "    #convert to hue, saturation, Value in openCV\n",
    "    hsv = cv2.cvtColor(img_BGR, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    # if less than prefined number of values are above a saturation threshold, return False\n",
    "    #this is typically the gray background around the biological object\n",
    "    if (s > sat_threshold).sum() < pixcount_th:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image():\n",
    "    def __init__(self, img, img_name =None):\n",
    "        self.img = img\n",
    "        self.shape = img.shape\n",
    "        self.name = img_name\n",
    "\n",
    "        self.image_reshape()\n",
    "        self.dx = self.shape[0]\n",
    "        self.dy = self.shape[1]\n",
    "\n",
    "        self.tile_size = None\n",
    "\n",
    "        self.pad_x = None\n",
    "        self.pad_y = None\n",
    "        self.tiled_img = None\n",
    "\n",
    "        self.mask_rle = None\n",
    "        self.mask_2d = None\n",
    "        self.tiled_mask = None\n",
    "        \n",
    "    \n",
    "    def image_reshape(self):\n",
    "        \n",
    "        if len(self.shape) == 5:\n",
    "            self.img = np.transpose(self.img.squeeze(), (1, 2, 0))\n",
    "            self.shape = self.img.shape\n",
    "            \n",
    "    \n",
    "    def split_image_mask_into_tiles(self, reduce=1, sz=512):\n",
    "     \n",
    "        self.tile_size = sz\n",
    "\n",
    "        self.pad_x, self.pad_y = get_padsize(self.img, reduce, sz)\n",
    "        print(self.pad_x, self.pad_y)\n",
    "        #Create padded Image and padded mask2D\n",
    "        img_padded  = np.pad(self.img, [self.pad_x, self.pad_y, (0, 0)], constant_values=0)\n",
    "        mask_padded = np.pad(self.mask_2d, [self.pad_x, self.pad_y], constant_values = 0)\n",
    "\n",
    "        print(\"shape of image after padding:: \", img_padded.shape,\n",
    "            img_padded.shape[0]//sz, img_padded.shape[1]//sz)\n",
    "\n",
    "        print(\"shape of mask after padding:: \", mask_padded.shape,\n",
    "              mask_padded.shape[0]//sz, mask_padded.shape[1]//sz)\n",
    "\n",
    "        #tile the padded image\n",
    "        img_reshaped = img_padded.reshape(\n",
    "            img_padded.shape[0]//sz, sz, img_padded.shape[1]//sz, sz, 3)\n",
    "        img_reshaped = img_reshaped.transpose(0, 2, 1, 3, 4).reshape(-1, sz, sz, 3)\n",
    "\n",
    "        #tile the padded mask2D\n",
    "        mask_reshaped = mask_padded.reshape(\n",
    "            mask_padded.shape[0]//sz, sz, mask_padded.shape[1]//sz, sz)\n",
    "        mask_reshaped = mask_reshaped.transpose(\n",
    "            0, 2, 1, 3).reshape(-1, sz, sz)\n",
    "\n",
    "        self.tiled_img = img_reshaped\n",
    "        self.tiled_mask = mask_reshaped\n",
    "            \n",
    "            \n",
    "    def save_thresholded_image(self, tiled_threshold_img_dir, mask_tile_dict, sat_threshold=40, pixcount_th=200):\n",
    "        \"\"\"\n",
    "        instead of saving, check thresholding of image\n",
    "        if it passes threshold then do an inference else predict a mask of all zeros\n",
    "        \"\"\"\n",
    "        \n",
    "        n = self.tiled_img.shape[0] \n",
    "\n",
    "        valid_img_count = 0\n",
    "        valid_idx = []\n",
    "        print(f\"Original tiled image count = {n}\")\n",
    "\n",
    "        for i in range(n):\n",
    "            img_BGR = self.tiled_img[i, :, :, :]\n",
    "            if utils.check_threshold(img_BGR, sat_threshold, pixcount_th):\n",
    "                valid_img_count += 1\n",
    "                valid_idx.append(i)\n",
    "                \n",
    "                #create an id for the image tile\n",
    "                img_tile_id = f\"{self.name}_{str(self.tile_size)}_{str(valid_img_count)}_{str(i)}\"\n",
    "                ###img_name = img_tile_id+'.png'  # name of the saved image tile\n",
    "\n",
    "                mask_for_tile = self.tiled_mask[i, :, :]  # get the mask for the tile\n",
    "                #convert the mask for the tile to rle\n",
    "                mask_rle = self.mask_2d_to_rle(mask_for_tile)\n",
    "                #save the rle mask to a dict, key = name of the corresponding image tile\n",
    "                mask_tile_dict[img_tile_id] = mask_rle\n",
    "            else:\n",
    "                mask_for_tile = np.zeros(mask_for_tile)\n",
    "                \n",
    "                \n",
    "                ###if valid_img_count == 1001:\n",
    "                ###cv2.imwrite(os.path.join(tiled_threshold_img_dir, img_name), img_BGR)\n",
    "\n",
    "        print(f\"Image count after thresholding = {valid_img_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __str__(self):\n",
    "        model_params = filter(lambda x: x.requires_grad, self.parameters())\n",
    "\n",
    "        return super(BaseModel, self).__str__()\n",
    "    \n",
    "    \n",
    "class Conv2x(nn.Module):\n",
    "    '''\n",
    "    preserves the the size of the image\n",
    "    '''\n",
    "    def __init__(self, in_ch, out_ch, inner_ch=None):\n",
    "        super(Conv2x, self).__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.inner_ch = out_ch//2 if inner_ch is None else inner_ch\n",
    "\n",
    "        self.conv2d_1 = nn.Conv2d(self.in_ch, self.inner_ch,\n",
    "                                  kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2d_2 = nn.Conv2d(self.inner_ch, self.out_ch,\n",
    "                                  kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inner_ch)\n",
    "        self.bn2 = nn.BatchNorm2d(self.out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(encoder, self).__init__()\n",
    "        self.conv2x = Conv2x(in_ch, out_ch)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, ceil_mode=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2x(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(decoder, self).__init__()\n",
    "        self.transposeconv = nn.ConvTranspose2d(\n",
    "            in_ch, in_ch//2, kernel_size=2, stride=2)\n",
    "        self.conv2x = Conv2x(in_ch, out_ch)\n",
    "\n",
    "\n",
    "    def forward(self, x_down, x_up, interpolate=True):\n",
    "\n",
    "        x_up = self.transposeconv(x_up)\n",
    "\n",
    "        #check for matching dims before concatenating\n",
    "\n",
    "        if (x_up.size(2) != x_up.size(2)) or (x_up.size(3) != x_up.size(3)):\n",
    "            if interpolate:\n",
    "                x_up = F.interpolate(x_up, size=(x_down.size(2), x_down.size(3)),\n",
    "                mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "        #Concat features from down conv channel and current up-conv\n",
    "        #along channel dim =1\n",
    "        x_up = torch.cat([x_up, x_down], dim=1) \n",
    "        x_up = self.conv2x(x_up)\n",
    "\n",
    "        return x_up\n",
    "    \n",
    "\n",
    "\n",
    "class UNet(BaseModel):\n",
    "\n",
    "    def __init__(self, in_ch=3, conv_channels=[16, 32, 64, 128, 256]):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.conv_channels = conv_channels\n",
    "        self.conv_start = Conv2x(in_ch, conv_channels[0]) #output_size = input_size\n",
    "        self.down1 = encoder(conv_channels[0], conv_channels[1])   #output_size = input_size/2\n",
    "        self.down2 = encoder(conv_channels[1], conv_channels[2])   #output_size = input_size/2\n",
    "        self.down3 = encoder(conv_channels[2], conv_channels[3])   #output_size = input_size/2\n",
    "        self.down4 = encoder(conv_channels[3], conv_channels[4])   #output_size = input_size/2\n",
    "\n",
    "        self.conv_middle = Conv2x(conv_channels[4], conv_channels[4]) #output_size = input_size\n",
    "\n",
    "        self.up4 = decoder(conv_channels[4], conv_channels[3]) #output_size = input_size * 2\n",
    "        self.up3 = decoder(conv_channels[3], conv_channels[2]) #output_size = input_size * 2\n",
    "        self.up2 = decoder(conv_channels[2], conv_channels[1]) #output_size = input_size * 2\n",
    "        self.up1 = decoder(conv_channels[1], conv_channels[0]) #output_size = input_size * 2\n",
    "\n",
    "        self.final_conv = nn.Conv2d(self.conv_channels[0], 1, kernel_size=1)\n",
    "\n",
    "        self.init_params()\n",
    "    \n",
    "    \n",
    "    def init_params(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                module.weight.data.fill_(1)\n",
    "                module.bias.data.zero_()\n",
    "                \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # size of x = [B, _, nx, ny]\n",
    "        \n",
    "        x1 = self.conv_start(x)  # size of x = [B, self.conv_channels[0], nx, ny]\n",
    "        x2 = self.down1(x1)  # size of x = [B, self.conv_channels[1], nx/2, ny/2]\n",
    "        x3 = self.down2(x2)  # size of x = [B, self.conv_channels[2], nx/4, ny/4]\n",
    "        x4 = self.down3(x3)  # size of x = [B, self.conv_channels[3], nx/8, ny/8]\n",
    "        x5 = self.down4(x4)  # size of x = [B, self.conv_channels[4], nx/16, ny/16]\n",
    "\n",
    "        x = self.conv_middle(x5)  # size of x = [B, self.conv_channels[4], nx/16, ny/16]\n",
    "\n",
    "        x = self.up4(x4, x)       # size of x = [B, self.conv_channels[3], nx/8, ny/8]\n",
    "        x = self.up3(x3, x)       # size of x = [B, self.conv_channels[2], nx/4, ny/4]\n",
    "        x = self.up2(x2, x)       # size of x = [B, self.conv_channels[1], nx/2, ny/2]\n",
    "        x = self.up1(x1, x)       # size of x = [B, self.conv_channels[0], nx, ny]\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_dice_iou(output, target, smooth = 0.005):\n",
    "        tp = (output * target).sum(axis=(1,2)) #intersection\n",
    "        fp = (output * (1.0 - target)).sum(axis=(1,2)) #false positives\n",
    "        fn = ((1.0 - output) * target).sum(axis=(1,2)) #false negatives\n",
    "        dice = np.mean((2.0 * tp + smooth) / (2 * tp + fp + fn + smooth))\n",
    "        iou = np.mean((tp + smooth) / (tp + fp + fn + smooth))\n",
    "\n",
    "        return dice, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer, checkpoint configs\n",
    "\n",
    "class BaseTrainer:\n",
    "\n",
    "    def __init__(self, model, loss, config, train_loader, val_loader):\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.config = config\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        self.start_epoch = 1\n",
    "\n",
    "        self.device = self.get_device()\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        #Training configs\n",
    "        cfg_train = config['trainer']\n",
    "        self.epochs = cfg_train['epochs']\n",
    "        self.save_period = cfg_train['save_period']\n",
    "\n",
    "        #Checkpoint configs\n",
    "        cur_dir = os.curdir\n",
    "        self.checkpoint_dir = os.path.join(cur_dir, cfg_train[\"save_dir\"])\n",
    "        print(self.checkpoint_dir)\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18484, 13013, 3)\n",
      "(230, 230) (149, 150)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pad() missing 1 required positional argument: 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-1184142ccc8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m#call this function for all the prediction images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0minference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_single_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-140-1184142ccc8d>\u001b[0m in \u001b[0;36minference_single_image\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mraw_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m#call Image.split_image_mask_into_tiles(self, reduce=1, sz=512) to create\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mraw_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_image_mask_into_tiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mimage_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtiled_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-11e2a547b8eb>\u001b[0m in \u001b[0;36msplit_image_mask_into_tiles\u001b[0;34m(self, reduce, sz)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#Create padded Image and padded mask2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mimg_padded\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mmask_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: pad() missing 1 required positional argument: 'mode'"
     ]
    }
   ],
   "source": [
    "def inference_single_image(img):\n",
    "    '''\n",
    "    if self.val_loader is None:\n",
    "        print(f\"No val loader exists\")\n",
    "        return {}\n",
    "    '''\n",
    "    model = UNet() \n",
    "\n",
    "    masks = '/Users/Ethan/Documents/Documents/Documents - Ethan’s MacBook Pro/python/kidney_files/hubmap-kidney-segmentation/train.csv'\n",
    "    df_masks = pd.read_csv(masks).set_index('id')\n",
    "    mask_image = df_masks.loc['aaa6a05cc']\n",
    "\n",
    "    # save checkpoint\n",
    "    checkpoint_path = '/Users/Ethan/Documents/Documents/Documents - Ethan’s MacBook Pro/python/kidney_files/hubmap-kidney-segmentation/model_checkpoints.pth'\n",
    "    store_model_checkpoint = torch.save(model.state_dict(),checkpoint_path)\n",
    "    \n",
    "    # load state\n",
    "    model.load_state_dict(torch.load(checkpoint_path)) \n",
    "\n",
    "    #resume checkpoint\n",
    "    #_resume_checkpoint(checkpoint_path)\n",
    "    \n",
    "    model.eval()\n",
    "    #model._reset_metrics()\n",
    "    #tbar = tqdm(self.val_loader, ncols=100)\n",
    "\n",
    "    \n",
    "    #read an image\n",
    "    datadir_train = '/Users/Ethan/Documents/Documents/Documents - Ethan’s MacBook Pro/python/kidney_files/hubmap-kidney-segmentation/train'\n",
    "    img_index = 6\n",
    "    raw_image_files = [f for f in os.listdir(datadir_train) if \"tiff\" in f]\n",
    "    f = raw_image_files[img_index]\n",
    "    raw_file_name = f.split('.')[0]\n",
    "    img_raw = tiff.imread(os.path.join(datadir_train, f))\n",
    "\n",
    "\n",
    "    #instantiate image class\n",
    "    raw_img = Image(img_raw,img_name=raw_file_name)\n",
    "    #call Image.split_image_mask_into_tiles(self, reduce=1, sz=512) to create\n",
    "    raw_img.split_image_mask_into_tiles(reduce=1, sz=512)\n",
    "\n",
    "    image_1 = raw_img.tiled_img\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        tile_count = image_1.shape[0]\n",
    "\n",
    "        total_tiles = 0\n",
    "        tiles_idx = []\n",
    "\n",
    "        accumulate_predictions = np.empty((tile_count,sz,sz))\n",
    "\n",
    "        #loop over the tiles\n",
    "        for i in range(tile_count):\n",
    "            img_BGR = tiled_img[i, :, :, :]\n",
    "            if utils.check_threshold(img_BGR, sat_threshold, pixcount_th):\n",
    "                total_tiles += 1\n",
    "                tiles_idx.append(i)\n",
    "                #for i, sample_batch in enumerate(tbar):\n",
    "                            #for each tile\n",
    "                            #check thresolhold\n",
    "                            #if threshold passes, then do inference\n",
    "                            ###img_tile_id = raw_img_name+'_'+str(sz)+'_'+str(total_tiles)\n",
    "                img = sample_batch['image']\n",
    "                #mask = sample_batch['mask'].float()\n",
    "\n",
    "                batch_size = img.shape[0]\n",
    "                img = img.to(self.device)\n",
    "                #mask = mask.to(self.device)\n",
    "\n",
    "                out = torch.squeeze(self.model(img), 1)\n",
    "                        #out = \n",
    "                        # convert to 0 or 1\n",
    "            else:\n",
    "                out = np.zeros(img)\n",
    "\n",
    "        #accumulate the predictions into a big np array\n",
    "        accumulate_predictions[i] = inference_single_image(image)\n",
    "        #prediction size = [total_tiles, sz, sz]\n",
    "\n",
    "        reconstructed_image = raw_img.reconstruct_original_from_padded_tiled_image(tiled_img)\n",
    "        # call above to get the predicted mask as the same size as the original image / mask\n",
    "\n",
    "        #calculate metrics\n",
    "        metric_dice_iou(tiled_img, raw_img)\n",
    "        #calculate 2d_to_rle\n",
    "        mask_pred_2d_to_rle = mask_2d_to_rle(reconstructed_image)\n",
    "\n",
    "        return mask_pred_2d_to_rle\n",
    "\n",
    "#call this function for all the prediction images\n",
    "inference = inference_single_image(image_1)\n",
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_checkpoints( epoch):\n",
    "        #create a state dict for saving\n",
    "\n",
    "        model_state = {\n",
    "            'epoch':epoch,\n",
    "            'state_dict':model.state_dict(),\n",
    "            'optimizer':optimizer.state_dict(),\n",
    "            'config':config\n",
    "        }\n",
    "        savetime = datetime.datetime.now().strftime('%m_%d_%H_%M')\n",
    "        filename = f\"{self.config['name']}_{epoch}_{savetime}\"\n",
    "        filename = os.path.join(checkpoint_dir, f'{filename}.pth')\n",
    "        torch.save(model_state, filename)\n",
    "\n",
    "def _resume_checkpoint(self):\n",
    "    pass\n",
    "\n",
    "def _train_epoch(self):\n",
    "        #implement this in Trainer (sub class of BaseTrainer) \n",
    "    raise NotImplementedError\n",
    "\n",
    "def _val_epoch(self, epoch):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resume_checkpoint(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path) #need this line\n",
    "\n",
    "        last_epoch = checkpoint['epoch']\n",
    "        model_name = checkpoint['config']['name']\n",
    "\n",
    "        if model_name == config['name']: # self was here\n",
    "            load_model = model.load_state_dict(checkpoint['state_dict']) #need this line\n",
    "            start_epoch = last_epoch + 1\n",
    "            #self removed from top two lines\n",
    "            return True\n",
    "        else:\n",
    "            print(\"current model name doesn't match with previously saved model name !!\")\n",
    "            print(\"Current model name: {} , previous model name : {}\".format(config['name'], model_name)) #originally self.config \n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reconstructed_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-d5995c0336c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"incorrect encoding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcomparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreconstructed_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reconstructed_image' is not defined"
     ]
    }
   ],
   "source": [
    "# read corresponding mask to compare to reconstructed  img\n",
    "def comparison(mask,image_reconstructed):\n",
    "    raw_img.mask_rle = df_masks.loc[raw_file_name, 'encoding']\n",
    "    actual_mask_rle = raw_img.mask_rle\n",
    "    if mask_pred_2d_to_rle == actual_mask_rle:\n",
    "        return True\n",
    "    else:\n",
    "        print(\"incorrect encoding\")\n",
    "    \n",
    "comparison(mask_image,reconstructed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_original_from_padded_tiled_image(self, tiled_image):\n",
    "        n = tiled_image.shape[0]\n",
    "        tile_size = self.tile_size\n",
    "        (pad_x_l, pad_x_r) = self.pad_x\n",
    "        (pad_y_l, pad_y_r) = self.pad_y\n",
    "\n",
    "        dx_padded = self.dx + pad_x_l + pad_x_r\n",
    "        dy_padded = self.dy + pad_y_l + pad_y_r\n",
    "\n",
    "        n_x = dx_padded //tile_size\n",
    "        n_y = dy_padded//tile_size\n",
    "\n",
    "        assert (n == n_x*n_y), \"dimensions don't match\"\n",
    "\n",
    "        image_untiled = tiled_image.reshape(n_x, n_y, tile_size, tile_size, 3)\n",
    "        image_untiled = image_untiled.transpose(0,2,1,3,4)\n",
    "        image_padded = image_untiled.reshape(n_x*tile_size, n_y*tile_size, 3)\n",
    "\n",
    "        image_unpadded = image_padded[pad_x_l: - pad_x_r, pad_y_l: -pad_y_r, :]\n",
    "\n",
    "        assert (self.dx == image_unpadded.shape[0]), \\\n",
    "            \"shape of original image doesn't match with unpadded image along dim = 0\"\n",
    "        assert (self.dy == image_unpadded.shape[1]), \\\n",
    "            \"shape of original image doesn't match with unpadded image along dim = 1\"\n",
    "\n",
    "        return image_unpadded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont really need this cell\n",
    "\n",
    "from skimage import io, transform\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Dataset_Image_mask(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, mean, std_dev, transform=None):\n",
    "        super(Dataset_Image_mask, self).__init__()\n",
    "        self.root_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.mask_dict = self.get_mask_dict()\n",
    "        self.img_name_list = list(self.mask_dict.keys())\n",
    "        self.len = self.__len__()\n",
    "        self.normalize = transforms.Normalize(mean, std_dev)\n",
    "\n",
    "        \n",
    "\n",
    "    def get_mask_dict(self):\n",
    "        '''\n",
    "        open the pickled file containing the dict of mask in rle format\n",
    "        returns\n",
    "        dict: key same as imgae file name\n",
    "        value: numpy array in rle format\n",
    "        '''\n",
    "        img_and_mask_files = os.listdir(self.root_dir)\n",
    "        mask_rle_file = [x for x in img_and_mask_files if \"mask\" in x][0]\n",
    "        mask_rle_dict = pickle.load(\n",
    "            open(os.path.join(self.root_dir, mask_rle_file), 'rb'))\n",
    "        return mask_rle_dict\n",
    "\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.img_name_list)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_name_list[idx]\n",
    "        img_file_name = os.path.join(self.root_dir, f\"{img_name}.png\")\n",
    "        img = io.imread(img_file_name)\n",
    "        #img = torch.tensor(img.transpose((2,0,1))).float()\n",
    "        #img = img.transpose(2, 0, 1)\n",
    "        \n",
    "        mask_rle_np = self.mask_dict[img_name]\n",
    "        #print(\"mask rle shape :: \", mask_rle_np.shape)\n",
    "        #print(f\"**\\n{mask_rle_np}\")\n",
    "        mask_rle = \" \".join(str(x) for x in mask_rle_np)\n",
    "        \n",
    "        mask_2d = utils.mask_rle_to_2d(mask_rle, 512, 512)\n",
    "        #augment image for training\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            mask_2d = self.transform(mask_2d)\n",
    "        \n",
    "        img = self.normalize(transforms.ToTensor()(img))\n",
    "        mask = torch.from_numpy(mask_2d).long()\n",
    "\n",
    "        sample = {'image': img, 'mask': mask, 'idx': img_name}\n",
    "        #sample = {'image': img}\n",
    "        return sample\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        \\n    def main():\\n        root_dir = \\'C:\\\\Scripts\\\\hubmap\\\\code\\'\\n\\n        data_dir = \\'C:\\\\Scripts\\\\hubmap\\\\train\\\\tiled_thresholded_512\\'\\n\\n        mean = [0.68912, 0.47454, 0.6486]\\n        std_dev = [0.13275, 0.23647, 0.15536]\\n\\n        #full dataset with training images and masks\\n        dataset = dataloader.Dataset_Image_mask(data_dir, mean, std_dev)\\n\\n        n_tot = dataset.len\\n\\n        #SplitS full dataset into train set and test set\\n        train_test_split = 0.8\\n        train_count = int(train_test_split * n_tot)\\n\\n        test_count = dataset.len - train_count\\n\\n        train_idx = list(np.random.choice(\\n            range(n_tot), train_count, replace=False))\\n        test_idx = list(set(range(n_tot)) - set(train_idx))\\n\\n        print(len(train_idx), len(test_idx), n_tot - len(train_idx) - len(test_idx))\\n\\n        train_ds = torch.utils.data.Subset(dataset, train_idx)\\n        test_ds = torch.utils.data.Subset(dataset, test_idx)\\n\\n        model = unet.UNet()\\n\\n        config = json.load(open(\\'config.json\\'))\\n        b_size = config[\"train_loader\"][\"args\"][\"batch_size\"]\\n        train_loader = DataLoader(\\n            train_ds, batch_size=b_size, shuffle=True, num_workers=0)\\n        b_size = config[\"val_loader\"][\"args\"][\"batch_size\"]\\n        val_loader = DataLoader(\\n            test_ds, batch_size=b_size, shuffle=True, num_workers=0)\\n\\n        trainer = Trainer(model, loss.loss_fn, config, train_loader, val_loader)\\n        print(f\"Trainining on device: {trainer.device}\")\\n\\n        trainer.train()\\n\\nif __name__ == \"__main__\":\\n    main()\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dont really need this cell\n",
    "\n",
    "#from base.base_trainer import BaseTrainer\n",
    "#from utils import dataloader, loss, metrics\n",
    "#from models import unet\n",
    "\n",
    "class Trainer(BaseTrainer):\n",
    "\n",
    "    def __init__(self, model, loss_fn, config, train_loader, val_loader=None):\n",
    "        super(Trainer, self).__init__(\n",
    "            model, loss_fn, config, train_loader, val_loader)\n",
    "\n",
    "        self.optimizer = loss.use_optimizer(model, config)\n",
    "        self.bce_dice_ratio = config['loss'][\"bce_dice_ratio\"]\n",
    "\n",
    "    def _train_epoch(self, epoch):\n",
    "        '''\n",
    "        train the model for one epoch\n",
    "        '''\n",
    "        self.model.train()\n",
    "        self._reset_metrics()\n",
    "        tbar = tqdm(self.train_loader, ncols = 100, miniters=50)\n",
    "\n",
    "        for i, sample_batch in enumerate(tbar):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            img = sample_batch['image']\n",
    "            mask = sample_batch['mask'].float()\n",
    "\n",
    "            batch_size = img.shape[0]\n",
    "\n",
    "            img = img.to(self.device)\n",
    "            mask = mask.to(self.device)\n",
    "            #img = img.transpose((0,3,1,2))\n",
    "            out = torch.squeeze(self.model(img), 1)\n",
    "            \n",
    "            train_loss = self.loss(out, mask, self.bce_dice_ratio)\n",
    "            self.total_loss.update(train_loss.item(), batch_size)\n",
    "\n",
    "            train_loss.backward() #perform backprop\n",
    "            self.optimizer.step() #update parameters\n",
    "\n",
    "            tbar.set_description( f\"Train: Epoch: {epoch}, Avg Loss: {self.total_loss.avg:.5f}\" )\n",
    "            #if (i % 50 == 0):\n",
    "            #    print(f\"epoch: {epoch}, batch : {i}, train loss: {train_loss.item(): .5f}, train average loss: {self.total_loss.avg: .5f}\")\n",
    "            #    break\n",
    "        return self.total_loss.avg\n",
    "\n",
    "\n",
    "    def _val_epoch(self, epoch):\n",
    "            \n",
    "            if self.val_loader is None:\n",
    "                print(f\"No val loader exists\")\n",
    "                return {}\n",
    "\n",
    "            self.model.eval()\n",
    "            self._reset_metrics()\n",
    "            tbar = tqdm(self.val_loader, ncols=100)\n",
    "            with torch.no_grad():\n",
    "                for i, sample_batch in enumerate(tbar):\n",
    "                    img = sample_batch['image']\n",
    "                    mask = sample_batch['mask'].float()\n",
    "\n",
    "                    batch_size = img.shape[0]\n",
    "                    img = img.to(self.device)\n",
    "                    mask = mask.to(self.device)\n",
    "\n",
    "                    out = torch.squeeze(self.model(img), 1)\n",
    "                    val_loss = self.loss(out, mask, self.bce_dice_ratio)\n",
    "                    self.total_loss.update(val_loss.item(), batch_size)\n",
    "                    #if (i%10 == 0):\n",
    "                    #    print(f\"epoch: {epoch}, batch : {i}, val loss: {val_loss.item()}, val average loss: {self.total_loss.avg}\")\n",
    "                    tbar.set_description(f\"Val: Epoch: {epoch}, Avg Loss: {self.total_loss.avg:.5f}\")\n",
    "\n",
    "            return self.total_loss.avg\n",
    "\n",
    "   \n",
    "    \n",
    "    def _reset_metrics(self):\n",
    "\n",
    "        self.total_loss = metrics.AverageMeter()\n",
    "        \n",
    "'''        \n",
    "    def main():\n",
    "        root_dir = 'C:\\Scripts\\hubmap\\code'\n",
    "\n",
    "        data_dir = 'C:\\Scripts\\hubmap\\\\train\\\\tiled_thresholded_512'\n",
    "\n",
    "        mean = [0.68912, 0.47454, 0.6486]\n",
    "        std_dev = [0.13275, 0.23647, 0.15536]\n",
    "\n",
    "        #full dataset with training images and masks\n",
    "        dataset = dataloader.Dataset_Image_mask(data_dir, mean, std_dev)\n",
    "\n",
    "        n_tot = dataset.len\n",
    "\n",
    "        #SplitS full dataset into train set and test set\n",
    "        train_test_split = 0.8\n",
    "        train_count = int(train_test_split * n_tot)\n",
    "\n",
    "        test_count = dataset.len - train_count\n",
    "\n",
    "        train_idx = list(np.random.choice(\n",
    "            range(n_tot), train_count, replace=False))\n",
    "        test_idx = list(set(range(n_tot)) - set(train_idx))\n",
    "\n",
    "        print(len(train_idx), len(test_idx), n_tot - len(train_idx) - len(test_idx))\n",
    "\n",
    "        train_ds = torch.utils.data.Subset(dataset, train_idx)\n",
    "        test_ds = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "        model = unet.UNet()\n",
    "\n",
    "        config = json.load(open('config.json'))\n",
    "        b_size = config[\"train_loader\"][\"args\"][\"batch_size\"]\n",
    "        train_loader = DataLoader(\n",
    "            train_ds, batch_size=b_size, shuffle=True, num_workers=0)\n",
    "        b_size = config[\"val_loader\"][\"args\"][\"batch_size\"]\n",
    "        val_loader = DataLoader(\n",
    "            test_ds, batch_size=b_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        trainer = Trainer(model, loss.loss_fn, config, train_loader, val_loader)\n",
    "        print(f\"Trainining on device: {trainer.device}\")\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
